ğŸš€ Async Transformer Framework
A high-performance, production-ready framework for serving any Transformer model with async processing, intelligent batching, caching, and multi-model support.
âœ¨ Key Features

ğŸ”„ Truly Async: Built on FastAPI with async/await for high concurrency
ğŸ¯ Universal Model Support: Works with any Transformer model (classification, generation, NER, Q&A, etc.)
ğŸ“¦ Intelligent Batching: Automatically batches requests for optimal GPU utilization
âš¡ Response Caching: Built-in cache with configurable TTL
ğŸ”Œ Dynamic Model Loading: Load/unload models on-the-fly without restart
ğŸ“Š Multi-Model Support: Run multiple models simultaneously
ğŸ›¡ï¸ Production Ready: Error handling, health checks, statistics
ğŸ³ Docker Support: Easy deployment with Docker & docker-compose

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP Request
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        FastAPI Layer            â”‚
â”‚  (Async request handling)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Inference Queue             â”‚
â”‚  â€¢ Request batching             â”‚
â”‚  â€¢ Cache management             â”‚
â”‚  â€¢ Async coordination           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Model Manager               â”‚
â”‚  â€¢ Dynamic model loading        â”‚
â”‚  â€¢ Multi-model support          â”‚
â”‚  â€¢ Resource management          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Installation
# Clone the repository
git clone <your-repo>
cd transformer-framework

# Install dependencies
pip install -r requirements.txt

# Prepare your models
mkdir -p models/sentiment_model
# Copy your model files to models/sentiment_model/

Running the Server
# Start the server
python main.py

# Or with uvicorn
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 1

First API Call

# Check health
curl http://localhost:8000/

# Make a prediction
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "I love this product!", "model_id": "sentiment"}'
